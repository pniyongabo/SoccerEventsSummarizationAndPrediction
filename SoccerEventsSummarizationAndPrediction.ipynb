{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoccerEventsSummarizationAndPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgJGMXWj_LD"
      },
      "source": [
        "\n",
        "CS 410/510: NLP Final Project\n",
        "Students: Patrick Niyongabo, Robert Handy, Hanin Alshalan\n",
        "\n",
        "## Soccer Events Summarization and Results Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWX19mGRjZhp"
      },
      "source": [
        "### 0. Introdution and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXSAM1Fqj3UK",
        "outputId": "03854980-847a-462a-f008-3bf5e611a858"
      },
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plot \n",
        "import collections\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnmbAdVTl0ZF",
        "outputId": "f4db9bc4-a7a0-4a8d-9d6f-0de28ce71370"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3TtMvbjP8U"
      },
      "source": [
        "### 1. Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOoPCXKik9la",
        "outputId": "3da9cc21-0814-4551-8239-223d031e0916"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "path = '/content/drive/MyDrive/ColabData/kaggle-world-cup-2018-tweets.csv'\n",
        "# filename = 'kaggle-world-cup-2018-tweets.csv'\n",
        "df = pd.read_csv(path)\n",
        "print(df.shape[0])\n",
        "print(type(df))\n",
        "\n",
        "# df = pd.read_csv(io.StringIO(uploaded['kaggle-world-cup-2018-tweets.csv'].decode('utf-8')))\n",
        "# df = pd.read_csv(io.StringIO('kaggle-world-cup-2018-tweets.csv').decode('utf-8')))\n",
        "# df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530000\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6veuFVH55QU"
      },
      "source": [
        "\n",
        "1.1 Remove Low Quality Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAjUR50I54h5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44f3bfc-e932-4ed0-aef0-cdd3037fa6b7"
      },
      "source": [
        "# Kinda just putting a couple filtering things here. It should probably get combined and cleaned up and inplace=True\n",
        "# only_en = df.drop(df[df['lang'] != 'en'].index, inplace = False)\n",
        "# longer_than_20 = df.drop(df[df['len'] < 20].index, inplace = False)\n",
        "\n",
        "# stringify and lower case everything \n",
        "df[\"Tweet\"] = df[\"Tweet\"].str.lower()\n",
        "print(df.shape[0])\n",
        "\n",
        "# remove duplicate tweets\n",
        "df.drop_duplicates(subset='Tweet', keep=\"last\", inplace=True)\n",
        "print(df.shape[0])\n",
        "\n",
        "# filter out short tweets (less than 60 chars)\n",
        "df['Tweet'] = df['Tweet'].astype('str')\n",
        "dd = df[df['Tweet'].apply(lambda x: len(x)>60)]\n",
        "print(dd.shape[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530000\n",
            "113073\n",
            "62779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTSri8MmdLr8",
        "outputId": "a77a2752-8134-453d-8e03-1761aec2e3b5"
      },
      "source": [
        "# create dictionary mapping games to tweets based on timestamps & hashtags\n",
        "import json\n",
        "json_file = open('/content/drive/MyDrive/ColabData/cupfinals.json') \n",
        "world_cup_games = json.load(json_file) \n",
        "#print(world_cup_games[\"URUPOR\"][\"finalscore\"])\n",
        "#print(type(world_cup_games))\n",
        "world_cup_games[\"URUPOR\"][\"tweets\"] = [\"test tweet1\", \"test tweet2\"]\n",
        "#print(world_cup_games[\"URUPOR\"])\n",
        "\n",
        "# create dictionary mapping tweets by day\n",
        "tweets_by_date = {}\n",
        "for index, row in dd.iterrows():\n",
        "  gametime = row[\"Date\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  if gamedate in tweets_by_date:\n",
        "    tweets_by_date[gamedate].append(row['Tweet'])\n",
        "  else:\n",
        "    tweets_by_date[gamedate] = [row['Tweet']]\n",
        "print(len(tweets_by_date[\"2018-07-15\"]))  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKC0Wm13FxZL",
        "outputId": "acf8dd66-79e9-412a-cacc-f57b4c2ff489"
      },
      "source": [
        "# print(len(tweets_by_date[\"2018-07-06\"]))  # KeyError\n",
        "# print(len(tweets_by_date[\"2018-07-05\"])) # KeyError\n",
        "sum_of_tweets = 0\n",
        "for key in tweets_by_date:  # there are missing dates. i.e.: no tweets in our data even though games happened\n",
        "  print(key + \" : \"+ str(len(tweets_by_date[key])))\n",
        "  sum_of_tweets += len(tweets_by_date[key])\n",
        "print(sum_of_tweets)                      "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-07-02 : 1011\n",
            "2018-07-01 : 13831\n",
            "2018-07-03 : 2243\n",
            "2018-07-04 : 1946\n",
            "2018-06-30 : 9353\n",
            "2018-06-29 : 134\n",
            "2018-07-10 : 11812\n",
            "2018-07-11 : 9092\n",
            "2018-07-15 : 13357\n",
            "62779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUeclia8Eu8g",
        "outputId": "9772b8d5-fa15-4958-f3db-c464fde05551"
      },
      "source": [
        "# assign tweets to a game if date of tweet match date of game\n",
        "# more filtering can be added. example: match hashtag in tweet with match hashtag\n",
        "for game in world_cup_games:\n",
        "  gametime = world_cup_games[game][\"starttime\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  print(game, gamedate)\n",
        "  if gamedate in tweets_by_date:\n",
        "    world_cup_games[game][\"tweets\"] = tweets_by_date[gamedate]\n",
        "# print(len(world_cup_games[\"FRACRO\"][\"tweets\"]))\n",
        "# print(len(world_cup_games[\"CROENG\"][\"tweets\"]))\n",
        "assert len(tweets_by_date[\"2018-07-15\"]) == len(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "assert len(tweets_by_date[\"2018-07-11\"]) == len(world_cup_games[\"CROENG\"][\"tweets\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URUPOR 2018-06-30\n",
            "FRAARG 2018-06-30\n",
            "ESPRUS 2018-07-01\n",
            "CRODEN 2018-07-01\n",
            "BRAMEX 2018-07-02\n",
            "BELJAP 2018-07-02\n",
            "SWESUI 2018-07-03\n",
            "COLENG 2018-07-03\n",
            "URUFRA 2018-07-06\n",
            "BRABEL 2018-07-06\n",
            "RUSCRO 2018-07-07\n",
            "SWEENG 2018-07-07\n",
            "FRABEL 2018-07-10\n",
            "CROENG 2018-07-11\n",
            "BELENG 2018-07-14\n",
            "FRACRO 2018-07-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxh2eiy5dRyF",
        "outputId": "661012d1-dbdc-459c-eab9-c8a98e61c79a"
      },
      "source": [
        "all_FRACRO_tweets = \". \".join(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "all_CROENG_tweets = \". \".join(world_cup_games[\"CROENG\"][\"tweets\"])\n",
        "all_BRAMEX_tweets = \". \".join(world_cup_games[\"BRAMEX\"][\"tweets\"])\n",
        "all_BELJAP_tweets = \". \".join(world_cup_games[\"BELJAP\"][\"tweets\"])\n",
        "all_SWESUI_tweets = \". \".join(world_cup_games[\"SWESUI\"][\"tweets\"])\n",
        "all_COLENG_tweets = \". \".join(world_cup_games[\"COLENG\"][\"tweets\"])\n",
        "all_BEFORE_tweets = \". \".join(tweets_by_date[\"2018-06-29\"])\n",
        "print(\"length of all_FRACRO_tweets = \" + str(len(all_FRACRO_tweets)))\n",
        "print(\"length of all_CROENG_tweets = \" + str(len(all_CROENG_tweets)))\n",
        "print(\"length of all_BRAMEX_tweets = \" + str(len(all_BRAMEX_tweets)))\n",
        "print(\"length of all_BELJAP_tweets = \" + str(len(all_BELJAP_tweets)))\n",
        "print(\"length of all_SWESUI_tweets = \" + str(len(all_SWESUI_tweets)))\n",
        "print(\"length of all_COLENG_tweets = \" + str(len(all_COLENG_tweets)))\n",
        "print(\"length of all_BEFORE_tweets = \" + str(len(all_BEFORE_tweets)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of all_FRACRO_tweets = 1500569\n",
            "length of all_CROENG_tweets = 969233\n",
            "length of all_BRAMEX_tweets = 112269\n",
            "length of all_BELJAP_tweets = 112269\n",
            "length of all_SWESUI_tweets = 247862\n",
            "length of all_COLENG_tweets = 247862\n",
            "length of all_BEFORE_tweets = 13044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y8jF1ou2dfL"
      },
      "source": [
        "### 2. Events Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr7f8BKqBSLE"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
        "from string import punctuation\r\n",
        "from heapq import nlargest\r\n",
        "from collections import defaultdict \r\n",
        "from textblob import TextBlob\r\n",
        "\r\n",
        "stopwords = list(STOP_WORDS)\r\n",
        "nlp = spacy.load('en')\r\n",
        "nlp.max_length = 1600000 # don't increase this or program will crash\r\n",
        "\r\n",
        "def events_score(sent):\r\n",
        "  score = 0\r\n",
        "  if \"score\" in sent or \"goal\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"red card\" in sent or \"penal\" in sent or \"pk\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"yellow card\" in sent or \"freekick\" in sent or \"booked\" in sent or \"booking\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"foul\" in sent or \"sub\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"half\" in sent or \"full\" in sent or \"over\" in sent or \"extra\" in sent or \"ht\" in sent or \"ft\" in sent:\r\n",
        "    score += 2\r\n",
        "  return score \r\n",
        "\r\n",
        "\r\n",
        "# Place All As A Function For Reuseability\r\n",
        "def text_summarizer(raw_docx):\r\n",
        "    raw_text = raw_docx\r\n",
        "    docx = nlp(raw_text)\r\n",
        "    stopwords = list(STOP_WORDS)\r\n",
        "    # Build Word Frequency\r\n",
        "    # word.text is tokenization in spacy\r\n",
        "    word_frequencies = defaultdict(int)  \r\n",
        "    for word in docx:  \r\n",
        "      if word.text not in stopwords:\r\n",
        "        word_frequencies[word.text] += 1\r\n",
        "\r\n",
        "\r\n",
        "    maximum_frequncy = max(word_frequencies.values())\r\n",
        "\r\n",
        "    for word in word_frequencies.keys():  \r\n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\r\n",
        "    # Sentence Tokens\r\n",
        "    sentence_list = [ sentence for sentence in docx.sents ]\r\n",
        "\r\n",
        "    # Calculate Sentence Score and Ranking\r\n",
        "    sentence_scores = defaultdict(int)\r\n",
        "    for sent in sentence_list:\r\n",
        "      sentence = sent.text.lower()\r\n",
        "      if len(sentence.split(' ')) < 20 and abs(TextBlob(sentence).sentiment.polarity) < 0.5:\r\n",
        "        sentence_scores[sent] = events_score(sentence) \r\n",
        "        for word in sent:\r\n",
        "          if word.text.lower() in word_frequencies.keys():\r\n",
        "            sentence_scores[sent] += word_frequencies[word.text.lower()]\r\n",
        "\r\n",
        "    # Find N Largest\r\n",
        "    summary_sentences = nlargest(5, sentence_scores, key=sentence_scores.get)\r\n",
        "    final_sentences = [ w.text for w in summary_sentences ]\r\n",
        "    summary = '\\n'.join(final_sentences)\r\n",
        "    #print(\"Original Document\\n\")\r\n",
        "    #print(raw_docx)\r\n",
        "    #print(\"Total Length:\",len(raw_docx))\r\n",
        "    #print('\\n\\nSummarized Document\\n')\r\n",
        "    #print(\"Summary Length:\",len(summary))\r\n",
        "    #print(\"Summary: \" + summary)\r\n",
        "    return summary"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FdiIhqi2lkC"
      },
      "source": [
        "### 3. Summarization Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP9AjI8-dRNJ",
        "outputId": "b7a643bb-4e37-4b74-f08a-357c72965857"
      },
      "source": [
        "# text_summarizer(all_BEFORE_tweets)\n",
        "print(\"Summary of BRAMEX: \" + text_summarizer(all_BRAMEX_tweets))\n",
        "print(\"Summary of COLENG: \" + text_summarizer(all_COLENG_tweets))\n",
        "print(\"Summary of CROENG: \" + text_summarizer(all_CROENG_tweets))\n",
        "print(\"Summary of FRACRO: \" + text_summarizer(all_FRACRO_tweets))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of BRAMEX: croatia denmark on penalties ivan rakitic scores winning spot kick as croatia reach last eight.\n",
            "beats champions in tense penalty shoot out with final scoreline of http.\n",
            "goal jorgensen pokes in shot past subasic after scramble from throw in denmark lead inside mins here.\n",
            "subasic is only the second goalkeeper to save three penalties in shootout at the.\n",
            "fifa world cup beat in the penalty shoot out after suspenseful draw.\n",
            "Summary of COLENG: england vs colombia eric dier says he needed to score winning penalty after missing chance in extra time.\n",
            "goallll harry kane puts the penalty kick straight down the middle cool as cucumber england lead colombia kane wi.\n",
            "only two england goalkeepers have saved penalty in shootout david seaman jordan pickford ht.\n",
            "goal carlos sanchez fouls harry kane from corner and the captain takes care of the penalty em.\n",
            "eric dier says he needed to score winning penalty after missing chance in extra ti.\n",
            "Summary of CROENG: ffs can not handle extra time and penalties england need to get another goal and quick.\n",
            "that pernicious foul smell permeating through twitter is shitting himself after that goal three lions one.\n",
            "what we need is for someone to foul us in the penalty box and score from that.\n",
            "would rather walk over lego than have this go to penalties please score.\n",
            "i would rather walk over lego than have this go to penalties please score.\n",
            "Summary of FRACRO: icymi antoine griezmann sends an unstoppable penalty past danijel subasic into left side of the goal.\n",
            "own goal and gifted penalty from croatias side is the clear indication establishment was involved and helped france.\n",
            "own goal handball penalty pogbas left and right mbappes goal first teenager to score in final match lloris skil.\n",
            "penalties rebound goals scored loftus cheeks goal harry kane did not really deserve the golden boot.\n",
            "after the final in herr mollenkopf sent telegram to fifa asking why questionable goals were not subjected to slow.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfOnQo-Mqzz1",
        "outputId": "8533a5ff-57b6-41f1-9a91-cd37df618a6d"
      },
      "source": [
        "sett = \"that pernicious foul smell permeating through twitter is shitting himself after that goal three lions one.\"\n",
        "TextBlob(sett).sentiment.polarity"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}
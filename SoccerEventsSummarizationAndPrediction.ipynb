{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoccerEventsSummarizationAndPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgJGMXWj_LD"
      },
      "source": [
        "\n",
        "CS 410/510: NLP Final Project\n",
        "Students: Patrick Niyongabo, Robert Handy, Hanin Alshalan\n",
        "\n",
        "## Soccer Events Summarization and Results Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWX19mGRjZhp"
      },
      "source": [
        "### 0. Introdution and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXSAM1Fqj3UK",
        "outputId": "4507cf92-6d06-4069-e760-0447f2f8fd8a"
      },
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plot \n",
        "import collections\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnmbAdVTl0ZF",
        "outputId": "2e1eb365-f707-4073-ea7a-b0eb95729363"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3TtMvbjP8U"
      },
      "source": [
        "### 1. Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOoPCXKik9la",
        "outputId": "765ab757-0508-4f62-f73d-79c8fecb3153"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "path = '/content/drive/MyDrive/ColabData/kaggle-world-cup-2018-tweets.csv'\n",
        "# filename = 'kaggle-world-cup-2018-tweets.csv'\n",
        "df = pd.read_csv(path)\n",
        "print(df.shape[0])\n",
        "\n",
        "# df = pd.read_csv(io.StringIO(uploaded['kaggle-world-cup-2018-tweets.csv'].decode('utf-8')))\n",
        "# df = pd.read_csv(io.StringIO('kaggle-world-cup-2018-tweets.csv').decode('utf-8')))\n",
        "# df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5boSG7KzaiLp",
        "outputId": "cfc7ae3e-e374-4d0b-cf7a-75f9d6097859"
      },
      "source": [
        "print(type(df))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6veuFVH55QU"
      },
      "source": [
        "\n",
        "1.1 Remove Low Quality Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAjUR50I54h5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f9c544-4a06-499e-a8df-e82d7a1a8264"
      },
      "source": [
        "# Kinda just putting a couple filtering things here. It should probably get combined and cleaned up and inplace=True\n",
        "# only_en = df.drop(df[df['lang'] != 'en'].index, inplace = False)\n",
        "# longer_than_20 = df.drop(df[df['len'] < 20].index, inplace = False)\n",
        "\n",
        "# lower case everything \n",
        "df[\"Tweet\"] = df[\"Tweet\"].str.lower()\n",
        "\n",
        "# filter out short tweets (less than 50 chars)\n",
        "df['Tweet'] = df['Tweet'].astype('str')\n",
        "dd = df[df['Tweet'].apply(lambda x: len(x)>80)]\n",
        "print(dd.shape[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTSri8MmdLr8",
        "outputId": "7a03a878-7325-4658-b394-746a6987942b"
      },
      "source": [
        "# create dictionary mapping games to tweets based on timestamps & hashtags\n",
        "import json\n",
        "json_file = open('/content/drive/MyDrive/ColabData/cupfinals.json') \n",
        "world_cup_games = json.load(json_file) \n",
        "#print(world_cup_games[\"URUPOR\"][\"finalscore\"])\n",
        "#print(type(world_cup_games))\n",
        "world_cup_games[\"URUPOR\"][\"tweets\"] = [\"test tweet1\", \"test tweet2\"]\n",
        "#print(world_cup_games[\"URUPOR\"])\n",
        "\n",
        "# create dictionary mapping tweets by day\n",
        "tweets_by_date = {}\n",
        "for index, row in dd.iterrows():\n",
        "  gametime = row[\"Date\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  if gamedate in tweets_by_date:\n",
        "    tweets_by_date[gamedate].append(row['Tweet'])\n",
        "  else:\n",
        "    tweets_by_date[gamedate] = [row['Tweet']]\n",
        "print(len(tweets_by_date[\"2018-07-15\"]))  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKC0Wm13FxZL",
        "outputId": "03bf7751-7b09-41a7-d581-b33be7398849"
      },
      "source": [
        "# print(len(tweets_by_date[\"2018-07-06\"]))  # KeyError\n",
        "# print(len(tweets_by_date[\"2018-07-05\"])) # KeyError\n",
        "\n",
        "for key in tweets_by_date:  # there are missing dates. i.e.: no tweets in our data even though games happened\n",
        "  print(key + \" : \"+ str(len(tweets_by_date[key])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-07-02 : 4251\n",
            "2018-07-01 : 38687\n",
            "2018-07-03 : 5970\n",
            "2018-07-04 : 6925\n",
            "2018-06-30 : 35541\n",
            "2018-06-29 : 192\n",
            "2018-07-10 : 28921\n",
            "2018-07-11 : 15300\n",
            "2018-07-15 : 59643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUeclia8Eu8g",
        "outputId": "1c3bbbb8-e8db-48a5-98d7-02055ffad09d"
      },
      "source": [
        "# assign tweets to a game if date of tweet match date of game\n",
        "# more filtering can be added. example: match hashtag in tweet with match hashtag\n",
        "for game in world_cup_games:\n",
        "  gametime = world_cup_games[game][\"starttime\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  print(game, gamedate)\n",
        "  if gamedate in tweets_by_date:\n",
        "    world_cup_games[game][\"tweets\"] = tweets_by_date[gamedate]\n",
        "# print(len(world_cup_games[\"FRACRO\"][\"tweets\"]))\n",
        "# print(len(world_cup_games[\"CROENG\"][\"tweets\"]))\n",
        "assert len(tweets_by_date[\"2018-07-15\"]) == len(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "assert len(tweets_by_date[\"2018-07-11\"]) == len(world_cup_games[\"CROENG\"][\"tweets\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URUPOR 2018-06-30\n",
            "FRAARG 2018-06-30\n",
            "ESPRUS 2018-07-01\n",
            "CRODEN 2018-07-01\n",
            "BRAMEX 2018-07-02\n",
            "BELJAP 2018-07-02\n",
            "SWESUI 2018-07-03\n",
            "COLENG 2018-07-03\n",
            "URUFRA 2018-07-06\n",
            "BRABEL 2018-07-06\n",
            "RUSCRO 2018-07-07\n",
            "SWEENG 2018-07-07\n",
            "FRABEL 2018-07-10\n",
            "CROENG 2018-07-11\n",
            "BELENG 2018-07-14\n",
            "FRACRO 2018-07-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxh2eiy5dRyF",
        "outputId": "e4860975-e175-4194-8247-d4e02e6e3a53"
      },
      "source": [
        "all_FRACRO_tweets = \" \".join(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "all_CROENG_tweets = \" \".join(world_cup_games[\"CROENG\"][\"tweets\"])\n",
        "all_BRAMEX_tweets = \" \".join(world_cup_games[\"BRAMEX\"][\"tweets\"])\n",
        "all_BELJAP_tweets = \" \".join(world_cup_games[\"BELJAP\"][\"tweets\"])\n",
        "all_SWESUI_tweets = \" \".join(world_cup_games[\"SWESUI\"][\"tweets\"])\n",
        "all_COLENG_tweets = \" \".join(world_cup_games[\"COLENG\"][\"tweets\"])\n",
        "all_BEFORE_tweets = \" \".join(tweets_by_date[\"2018-06-29\"])\n",
        "print(\"length of all_FRACRO_tweets = \" + str(len(all_FRACRO_tweets)))\n",
        "print(\"length of all_CROENG_tweets = \" + str(len(all_CROENG_tweets)))\n",
        "print(\"length of all_BRAMEX_tweets = \" + str(len(all_BRAMEX_tweets)))\n",
        "print(\"length of all_BELJAP_tweets = \" + str(len(all_BELJAP_tweets)))\n",
        "print(\"length of all_SWESUI_tweets = \" + str(len(all_SWESUI_tweets)))\n",
        "print(\"length of all_COLENG_tweets = \" + str(len(all_COLENG_tweets)))\n",
        "print(\"length of all_BEFORE_tweets = \" + str(len(all_BEFORE_tweets)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of all_FRACRO_tweets = 6205172\n",
            "length of all_CROENG_tweets = 1671357\n",
            "length of all_BRAMEX_tweets = 439046\n",
            "length of all_BELJAP_tweets = 439046\n",
            "length of all_SWESUI_tweets = 631607\n",
            "length of all_COLENG_tweets = 631607\n",
            "length of all_BEFORE_tweets = 20374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr7f8BKqBSLE"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
        "from string import punctuation\r\n",
        "from heapq import nlargest\r\n",
        "\r\n",
        "stopwords = list(STOP_WORDS)\r\n",
        "nlp = spacy.load('en')\r\n",
        "\r\n",
        "# Place All As A Function For Reuseability\r\n",
        "def text_summarizer(raw_docx):\r\n",
        "    raw_text = raw_docx\r\n",
        "    docx = nlp(raw_text)\r\n",
        "    stopwords = list(STOP_WORDS)\r\n",
        "    # Build Word Frequency\r\n",
        "    # word.text is tokenization in spacy\r\n",
        "    word_frequencies = {}  \r\n",
        "    for word in docx:  \r\n",
        "        if word.text not in stopwords:\r\n",
        "            if word.text not in word_frequencies.keys():\r\n",
        "                word_frequencies[word.text] = 1\r\n",
        "            else:\r\n",
        "                word_frequencies[word.text] += 1\r\n",
        "\r\n",
        "\r\n",
        "    maximum_frequncy = max(word_frequencies.values())\r\n",
        "\r\n",
        "    for word in word_frequencies.keys():  \r\n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\r\n",
        "    # Sentence Tokens\r\n",
        "    sentence_list = [ sentence for sentence in docx.sents ]\r\n",
        "\r\n",
        "    # Calculate Sentence Score and Ranking\r\n",
        "    sentence_scores = {}  \r\n",
        "    for sent in sentence_list:  \r\n",
        "        for word in sent:\r\n",
        "            if word.text.lower() in word_frequencies.keys():\r\n",
        "                if len(sent.text.split(' ')) < 30:\r\n",
        "                    if sent not in sentence_scores.keys():\r\n",
        "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\r\n",
        "                    else:\r\n",
        "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\r\n",
        "\r\n",
        "    # Find N Largest\r\n",
        "    summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\r\n",
        "    final_sentences = [ w.text for w in summary_sentences ]\r\n",
        "    summary = ' '.join(final_sentences)\r\n",
        "    #print(\"Original Document\\n\")\r\n",
        "    #print(raw_docx)\r\n",
        "    #print(\"Total Length:\",len(raw_docx))\r\n",
        "    #print('\\n\\nSummarized Document\\n')\r\n",
        "    #print(\"Summary Length:\",len(summary))\r\n",
        "    #print(\"Summary: \" + summary)\r\n",
        "    return summary"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP9AjI8-dRNJ",
        "outputId": "a8ca1c6e-047f-4821-ce73-5cd9ac488141"
      },
      "source": [
        "# text_summarizer(all_FRACRO_tweets)\n",
        "# text_summarizer(all_CROENG_tweets)\n",
        "print(\"Summary of BRAMEX: \" + text_summarizer(all_BRAMEX_tweets))\n",
        "print(\"Summary of COLENG: \" + text_summarizer(all_COLENG_tweets))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of BRAMEX: all kasper schmeichel vs croatia saves penalty in extra time saves penalties in shootout still knocked out looking to celebrate your teams win at the fifa world cup russiatm kasper schmeichel vs croatia saves penalty in extra time saves penalties in shootout still knocked out of the route to final round of colombia quarter saves penalty in extra time saves penalties in shootout still knocked out of the route to final round of colombia quarter finals switzerland or sweden semi finals\n",
            "Summary of COLENG: quarter final fixtures friday uruguay vs france brazil vs belgium saturday england vs retweet if your country are still in the russia uruguay france croatia sweden brazil belgiu southgate salute club and country leader intelligent diligent dignified respectful passionate rocks waistcoa quarter finals friday uruguay vs france brazil vs belgium saturday england vs swede england belgiu quarter finals friday uruguay vs france brazil vs belgium saturday england vs swede sheffields harry maguire managed to find himself blades flag as he celebrated englands win\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X6FCgthjfel"
      },
      "source": [
        "### 2. Events Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InHj0DuSxMEQ"
      },
      "source": [
        "# keyword # number of mentions # timestamp\n",
        "# goal    # 100 tweets         # 20th june 2018 at 1pm\n",
        "\n",
        "# "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7N_DDCsjoEC"
      },
      "source": [
        "### 3. Results Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OKW93q4yJR-"
      },
      "source": [
        "# manually using sentence templates\n",
        "# automatically using text generator NLP tools"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHGmrv0Mjuqe"
      },
      "source": [
        "### 4. Events Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsTIGLLr_Q7o"
      },
      "source": [
        "# keyword # number of mentions # timestamp\n",
        "# goal    # 100 tweets         # 20th june 2018 at 1pm\n",
        "\n",
        "# "
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}
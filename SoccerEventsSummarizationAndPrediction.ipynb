{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoccerEventsSummarizationAndPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgJGMXWj_LD"
      },
      "source": [
        "\n",
        "CS 410/510: NLP Final Project\n",
        "Students: Patrick Niyongabo, Robert Handy, Hanin Alshalan\n",
        "\n",
        "## Soccer Events Summarization and Results Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWX19mGRjZhp"
      },
      "source": [
        "### 0. Introdution and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXSAM1Fqj3UK",
        "outputId": "b57c50ba-1067-4e43-81a6-158cc9618343"
      },
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plot \n",
        "import collections\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnmbAdVTl0ZF",
        "outputId": "ab80e093-0427-4643-ba5d-63a8527075dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3TtMvbjP8U"
      },
      "source": [
        "### 1. Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOoPCXKik9la",
        "outputId": "f9ebf646-0f39-42d1-a120-f536e4030d79"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "path = '/content/drive/MyDrive/ColabData/kaggle-world-cup-2018-tweets.csv'\n",
        "# filename = 'kaggle-world-cup-2018-tweets.csv'\n",
        "df = pd.read_csv(path)\n",
        "print(df.shape[0])\n",
        "print(type(df))\n",
        "\n",
        "# df = pd.read_csv(io.StringIO(uploaded['kaggle-world-cup-2018-tweets.csv'].decode('utf-8')))\n",
        "# df = pd.read_csv(io.StringIO('kaggle-world-cup-2018-tweets.csv').decode('utf-8')))\n",
        "# df"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530000\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6veuFVH55QU"
      },
      "source": [
        "\n",
        "1.1 Remove Low Quality Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAjUR50I54h5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9ca98a-20cc-478c-b41d-2f43b2d70d28"
      },
      "source": [
        "# Kinda just putting a couple filtering things here. It should probably get combined and cleaned up and inplace=True\n",
        "# only_en = df.drop(df[df['lang'] != 'en'].index, inplace = False)\n",
        "# longer_than_20 = df.drop(df[df['len'] < 20].index, inplace = False)\n",
        "\n",
        "# stringify and lower case everything \n",
        "df[\"Tweet\"] = df[\"Tweet\"].str.lower()\n",
        "print(df.shape[0])\n",
        "\n",
        "# remove duplicate tweets\n",
        "df.drop_duplicates(subset='Tweet', keep=\"last\", inplace=True)\n",
        "print(df.shape[0])\n",
        "\n",
        "# filter out short tweets (less than 60 chars)\n",
        "df['Tweet'] = df['Tweet'].astype('str')\n",
        "dd = df[df['Tweet'].apply(lambda x: len(x)>60)]\n",
        "print(dd.shape[0])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530000\n",
            "113073\n",
            "62779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTSri8MmdLr8",
        "outputId": "6b114e75-e1cc-47a2-a3b4-b86fe020a27c"
      },
      "source": [
        "# create dictionary mapping games to tweets based on timestamps & hashtags\n",
        "import json\n",
        "json_file = open('/content/drive/MyDrive/ColabData/cupfinals.json') \n",
        "world_cup_games = json.load(json_file) \n",
        "#print(world_cup_games[\"URUPOR\"][\"finalscore\"])\n",
        "#print(type(world_cup_games))\n",
        "world_cup_games[\"URUPOR\"][\"tweets\"] = [\"test tweet1\", \"test tweet2\"]\n",
        "#print(world_cup_games[\"URUPOR\"])\n",
        "\n",
        "# create dictionary mapping tweets by day\n",
        "tweets_by_date = {}\n",
        "for index, row in dd.iterrows():\n",
        "  gametime = row[\"Date\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  if gamedate in tweets_by_date:\n",
        "    tweets_by_date[gamedate].append(row['Tweet'])\n",
        "  else:\n",
        "    tweets_by_date[gamedate] = [row['Tweet']]\n",
        "print(len(tweets_by_date[\"2018-07-15\"]))  "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKC0Wm13FxZL",
        "outputId": "6f0fc287-8fad-428d-9a1e-38831e4b33a4"
      },
      "source": [
        "# print(len(tweets_by_date[\"2018-07-06\"]))  # KeyError\n",
        "# print(len(tweets_by_date[\"2018-07-05\"])) # KeyError\n",
        "sum_of_tweets = 0\n",
        "for key in tweets_by_date:  # there are missing dates. i.e.: no tweets in our data even though games happened\n",
        "  print(key + \" : \"+ str(len(tweets_by_date[key])))\n",
        "  sum_of_tweets += len(tweets_by_date[key])\n",
        "print(sum_of_tweets)                      "
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-07-02 : 1011\n",
            "2018-07-01 : 13831\n",
            "2018-07-03 : 2243\n",
            "2018-07-04 : 1946\n",
            "2018-06-30 : 9353\n",
            "2018-06-29 : 134\n",
            "2018-07-10 : 11812\n",
            "2018-07-11 : 9092\n",
            "2018-07-15 : 13357\n",
            "62779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUeclia8Eu8g",
        "outputId": "e7994821-2511-4727-bb85-f626c3152978"
      },
      "source": [
        "# assign tweets to a game if date of tweet match date of game\n",
        "# more filtering can be added. example: match hashtag in tweet with match hashtag\n",
        "for game in world_cup_games:\n",
        "  gametime = world_cup_games[game][\"starttime\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  print(game, gamedate)\n",
        "  if gamedate in tweets_by_date:\n",
        "    world_cup_games[game][\"tweets\"] = tweets_by_date[gamedate]\n",
        "# print(len(world_cup_games[\"FRACRO\"][\"tweets\"]))\n",
        "# print(len(world_cup_games[\"CROENG\"][\"tweets\"]))\n",
        "assert len(tweets_by_date[\"2018-07-15\"]) == len(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "assert len(tweets_by_date[\"2018-07-11\"]) == len(world_cup_games[\"CROENG\"][\"tweets\"])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URUPOR 2018-06-30\n",
            "FRAARG 2018-06-30\n",
            "ESPRUS 2018-07-01\n",
            "CRODEN 2018-07-01\n",
            "BRAMEX 2018-07-02\n",
            "BELJAP 2018-07-02\n",
            "SWESUI 2018-07-03\n",
            "COLENG 2018-07-03\n",
            "URUFRA 2018-07-06\n",
            "BRABEL 2018-07-06\n",
            "RUSCRO 2018-07-07\n",
            "SWEENG 2018-07-07\n",
            "FRABEL 2018-07-10\n",
            "CROENG 2018-07-11\n",
            "BELENG 2018-07-14\n",
            "FRACRO 2018-07-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxh2eiy5dRyF",
        "outputId": "25e0aa1e-5986-4365-db70-331a98d9c046"
      },
      "source": [
        "all_FRACRO_tweets = \". \".join(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "all_CROENG_tweets = \". \".join(world_cup_games[\"CROENG\"][\"tweets\"])\n",
        "all_BRAMEX_tweets = \". \".join(world_cup_games[\"BRAMEX\"][\"tweets\"])\n",
        "all_BELJAP_tweets = \". \".join(world_cup_games[\"BELJAP\"][\"tweets\"])\n",
        "all_SWESUI_tweets = \". \".join(world_cup_games[\"SWESUI\"][\"tweets\"])\n",
        "all_COLENG_tweets = \". \".join(world_cup_games[\"COLENG\"][\"tweets\"])\n",
        "all_BEFORE_tweets = \". \".join(tweets_by_date[\"2018-06-29\"])\n",
        "print(\"length of all_FRACRO_tweets = \" + str(len(all_FRACRO_tweets)))\n",
        "print(\"length of all_CROENG_tweets = \" + str(len(all_CROENG_tweets)))\n",
        "print(\"length of all_BRAMEX_tweets = \" + str(len(all_BRAMEX_tweets)))\n",
        "print(\"length of all_BELJAP_tweets = \" + str(len(all_BELJAP_tweets)))\n",
        "print(\"length of all_SWESUI_tweets = \" + str(len(all_SWESUI_tweets)))\n",
        "print(\"length of all_COLENG_tweets = \" + str(len(all_COLENG_tweets)))\n",
        "print(\"length of all_BEFORE_tweets = \" + str(len(all_BEFORE_tweets)))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of all_FRACRO_tweets = 1500569\n",
            "length of all_CROENG_tweets = 969233\n",
            "length of all_BRAMEX_tweets = 112269\n",
            "length of all_BELJAP_tweets = 112269\n",
            "length of all_SWESUI_tweets = 247862\n",
            "length of all_COLENG_tweets = 247862\n",
            "length of all_BEFORE_tweets = 13044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y8jF1ou2dfL"
      },
      "source": [
        "### 2. Events Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr7f8BKqBSLE"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
        "from string import punctuation\r\n",
        "from heapq import nlargest\r\n",
        "from collections import defaultdict \r\n",
        "\r\n",
        "stopwords = list(STOP_WORDS)\r\n",
        "nlp = spacy.load('en')\r\n",
        "nlp.max_length = 1600000 # don't increase this or program will crash\r\n",
        "\r\n",
        "def events_score(sent):\r\n",
        "  score = 0\r\n",
        "  if \"score\" in sent or \"goal\" in sent:\r\n",
        "    score += 5\r\n",
        "  if \"red card\" in sent or \"penal\" in sent or \"pk\" in sent:\r\n",
        "    score += 4\r\n",
        "  if \"yellow card\" in sent or \"freekick\" in sent or \"booked\" in sent or \"booking\" in sent:\r\n",
        "    score += 3\r\n",
        "  if \"foul\" in sent or \"sub\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"half\" in sent or \"full\" in sent or \"over\" in sent or \"extra\" in sent or \"ht\" in sent or \"ft\" in sent:\r\n",
        "    score += 2\r\n",
        "  return score \r\n",
        "\r\n",
        "\r\n",
        "# Place All As A Function For Reuseability\r\n",
        "def text_summarizer(raw_docx):\r\n",
        "    raw_text = raw_docx\r\n",
        "    docx = nlp(raw_text)\r\n",
        "    stopwords = list(STOP_WORDS)\r\n",
        "    # Build Word Frequency\r\n",
        "    # word.text is tokenization in spacy\r\n",
        "    word_frequencies = defaultdict(int)  \r\n",
        "    for word in docx:  \r\n",
        "      if word.text not in stopwords:\r\n",
        "        word_frequencies[word.text] += 1\r\n",
        "\r\n",
        "\r\n",
        "    maximum_frequncy = max(word_frequencies.values())\r\n",
        "\r\n",
        "    for word in word_frequencies.keys():  \r\n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\r\n",
        "    # Sentence Tokens\r\n",
        "    sentence_list = [ sentence for sentence in docx.sents ]\r\n",
        "\r\n",
        "    # Calculate Sentence Score and Ranking\r\n",
        "    sentence_scores = defaultdict(int)\r\n",
        "    for sent in sentence_list:\r\n",
        "      if len(sent.text.split(' ')) < 15: # 10\r\n",
        "        sentence_scores[sent] = events_score(sent.text.lower()) \r\n",
        "        for word in sent:\r\n",
        "          if word.text.lower() in word_frequencies.keys():\r\n",
        "            sentence_scores[sent] += word_frequencies[word.text.lower()]\r\n",
        "\r\n",
        "    # Find N Largest\r\n",
        "    summary_sentences = nlargest(5, sentence_scores, key=sentence_scores.get) # 7\r\n",
        "    final_sentences = [ w.text for w in summary_sentences ]\r\n",
        "    summary = '\\n'.join(final_sentences)\r\n",
        "    #print(\"Original Document\\n\")\r\n",
        "    #print(raw_docx)\r\n",
        "    #print(\"Total Length:\",len(raw_docx))\r\n",
        "    #print('\\n\\nSummarized Document\\n')\r\n",
        "    #print(\"Summary Length:\",len(summary))\r\n",
        "    #print(\"Summary: \" + summary)\r\n",
        "    return summary"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FdiIhqi2lkC"
      },
      "source": [
        "### 3. Summarization Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP9AjI8-dRNJ",
        "outputId": "4a1c0d8f-950f-4116-da76-347c4336855a"
      },
      "source": [
        "# text_summarizer(all_BEFORE_tweets)\n",
        "print(\"Summary of BRAMEX: \" + text_summarizer(all_BRAMEX_tweets))\n",
        "print(\"Summary of COLENG: \" + text_summarizer(all_COLENG_tweets))\n",
        "print(\"Summary of CROENG: \" + text_summarizer(all_CROENG_tweets))\n",
        "print(\"Summary of FRACRO: \" + text_summarizer(all_FRACRO_tweets))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of BRAMEX: beats champions in tense penalty shoot out with final scoreline of http.\n",
            "subasic is only the second goalkeeper to save three penalties in shootout at the.\n",
            "half time both team scored early minutes goals thanks to and here are the.\n",
            "how many more goals do you think will light up the second half.\n",
            "world cup croatia dump denmark in breathtaking penalty shootout with record saves.\n",
            "Summary of COLENG: only two england goalkeepers have saved penalty in shootout david seaman jordan pickford ht.\n",
            "eric dier reveals why he felt extra pressure to score his winning penalty.\n",
            "what is the shortest combined height of two penalty shootout goalkeepers.\n",
            "look back at that extraordinary game as late goal ended run match analysis.\n",
            "honestly everyone forgetting about the pickford save because colombia scored straight after.\n",
            "Summary of CROENG: would rather walk over lego than have this go to penalties please score.\n",
            "i would rather walk over lego than have this go to penalties please score.\n",
            "am calling this goes all the way to penalties the score after minutes\n",
            "croatia england perisic header levels score in second half of world cup semi final.\n",
            "ivan perisic goal for croatia croatia england after th minute dynamic second half.\n",
            "Summary of FRACRO: the own goal and the penalty kick really did them in overall fantastic game.\n",
            "an own goal and careless penalty not exactly the half were hoping for lead.\n",
            "tonight featuring france nd youngest world cup goal scorer croatia vs france in final.\n",
            "goals assists yellow cards red cards\n",
            "becomes the second teenager after pele to score in world cup final.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
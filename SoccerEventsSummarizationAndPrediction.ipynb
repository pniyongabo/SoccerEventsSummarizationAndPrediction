{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoccerEventsSummarizationAndPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgJGMXWj_LD"
      },
      "source": [
        "\n",
        "CS 410/510: NLP Final Project\n",
        "Students: Patrick Niyongabo, Robert Handy, Hanin Alshalan\n",
        "\n",
        "## Soccer Events Summarization and Results Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWX19mGRjZhp"
      },
      "source": [
        "### 0. Introdution and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXSAM1Fqj3UK",
        "outputId": "67305801-952c-4725-a041-a132690a1d46"
      },
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plot \n",
        "import collections\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnmbAdVTl0ZF",
        "outputId": "d2370dc4-7efb-4e7d-9d90-8b64db707568"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3TtMvbjP8U"
      },
      "source": [
        "### 1. Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOoPCXKik9la",
        "outputId": "8013aa22-637d-475b-a29b-2cadcfd72e86"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "path = '/content/drive/MyDrive/ColabData/kaggle-world-cup-2018-tweets.csv'\n",
        "# filename = 'kaggle-world-cup-2018-tweets.csv'\n",
        "df = pd.read_csv(path)\n",
        "print(df.shape[0])\n",
        "print(type(df))\n",
        "\n",
        "# df = pd.read_csv(io.StringIO(uploaded['kaggle-world-cup-2018-tweets.csv'].decode('utf-8')))\n",
        "# df = pd.read_csv(io.StringIO('kaggle-world-cup-2018-tweets.csv').decode('utf-8')))\n",
        "# df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530000\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6veuFVH55QU"
      },
      "source": [
        "\n",
        "1.1 Remove Low Quality Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAjUR50I54h5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49c008d-4e61-4e4c-ec1c-bdfd698ec6ac"
      },
      "source": [
        "# Kinda just putting a couple filtering things here. It should probably get combined and cleaned up and inplace=True\n",
        "# only_en = df.drop(df[df['lang'] != 'en'].index, inplace = False)\n",
        "# longer_than_20 = df.drop(df[df['len'] < 20].index, inplace = False)\n",
        "\n",
        "# stringify and lower case everything \n",
        "df[\"Tweet\"] = df[\"Tweet\"].str.lower()\n",
        "print(df.shape[0])\n",
        "\n",
        "# remove duplicate tweets\n",
        "df.drop_duplicates(subset='Tweet', keep=\"last\", inplace=True)\n",
        "print(df.shape[0])\n",
        "\n",
        "# filter out short tweets (less than 60 chars)\n",
        "df['Tweet'] = df['Tweet'].astype('str')\n",
        "dd = df[df['Tweet'].apply(lambda x: len(x)>60)]\n",
        "print(dd.shape[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530000\n",
            "113073\n",
            "62779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTSri8MmdLr8",
        "outputId": "95e87e0a-2f58-4924-8b5d-4622599ff977"
      },
      "source": [
        "# create dictionary mapping games to tweets based on timestamps & hashtags\n",
        "import json\n",
        "json_file = open('/content/drive/MyDrive/ColabData/cupfinals.json') \n",
        "world_cup_games = json.load(json_file) \n",
        "#print(world_cup_games[\"URUPOR\"][\"finalscore\"])\n",
        "#print(type(world_cup_games))\n",
        "world_cup_games[\"URUPOR\"][\"tweets\"] = [\"test tweet1\", \"test tweet2\"]\n",
        "#print(world_cup_games[\"URUPOR\"])\n",
        "\n",
        "# create dictionary mapping tweets by day\n",
        "tweets_by_date = {}\n",
        "for index, row in dd.iterrows():\n",
        "  gametime = row[\"Date\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  if gamedate in tweets_by_date:\n",
        "    tweets_by_date[gamedate].append(row['Tweet'])\n",
        "  else:\n",
        "    tweets_by_date[gamedate] = [row['Tweet']]\n",
        "print(len(tweets_by_date[\"2018-07-15\"]))  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKC0Wm13FxZL",
        "outputId": "83df2a62-e5bf-47b9-d97d-3faa18d96db9"
      },
      "source": [
        "# print(len(tweets_by_date[\"2018-07-06\"]))  # KeyError\n",
        "# print(len(tweets_by_date[\"2018-07-05\"])) # KeyError\n",
        "sum_of_tweets = 0\n",
        "for key in tweets_by_date:  # there are missing dates. i.e.: no tweets in our data even though games happened\n",
        "  print(key + \" : \"+ str(len(tweets_by_date[key])))\n",
        "  sum_of_tweets += len(tweets_by_date[key])\n",
        "print(sum_of_tweets)                      "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-07-02 : 1011\n",
            "2018-07-01 : 13831\n",
            "2018-07-03 : 2243\n",
            "2018-07-04 : 1946\n",
            "2018-06-30 : 9353\n",
            "2018-06-29 : 134\n",
            "2018-07-10 : 11812\n",
            "2018-07-11 : 9092\n",
            "2018-07-15 : 13357\n",
            "62779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUeclia8Eu8g",
        "outputId": "ecfd7d74-0d9b-4532-f8d5-c07a6a450cf8"
      },
      "source": [
        "# assign tweets to a game if date of tweet match date of game\n",
        "# more filtering can be added. example: match hashtag in tweet with match hashtag\n",
        "for game in world_cup_games:\n",
        "  gametime = world_cup_games[game][\"starttime\"]\n",
        "  gamedate = gametime.split(\" \")[0]\n",
        "  print(game, gamedate)\n",
        "  if gamedate in tweets_by_date:\n",
        "    world_cup_games[game][\"tweets\"] = tweets_by_date[gamedate]\n",
        "# print(len(world_cup_games[\"FRACRO\"][\"tweets\"]))\n",
        "# print(len(world_cup_games[\"CROENG\"][\"tweets\"]))\n",
        "assert len(tweets_by_date[\"2018-07-15\"]) == len(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "assert len(tweets_by_date[\"2018-07-11\"]) == len(world_cup_games[\"CROENG\"][\"tweets\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URUPOR 2018-06-30\n",
            "FRAARG 2018-06-30\n",
            "ESPRUS 2018-07-01\n",
            "CRODEN 2018-07-01\n",
            "BRAMEX 2018-07-02\n",
            "BELJAP 2018-07-02\n",
            "SWESUI 2018-07-03\n",
            "COLENG 2018-07-03\n",
            "URUFRA 2018-07-06\n",
            "BRABEL 2018-07-06\n",
            "RUSCRO 2018-07-07\n",
            "SWEENG 2018-07-07\n",
            "FRABEL 2018-07-10\n",
            "CROENG 2018-07-11\n",
            "BELENG 2018-07-14\n",
            "FRACRO 2018-07-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxh2eiy5dRyF",
        "outputId": "08d9b7fa-0469-4ca8-c051-b873dbc27d27"
      },
      "source": [
        "all_FRACRO_tweets = \". \".join(world_cup_games[\"FRACRO\"][\"tweets\"])\n",
        "all_CROENG_tweets = \". \".join(world_cup_games[\"CROENG\"][\"tweets\"])\n",
        "all_BRAMEX_tweets = \". \".join(world_cup_games[\"BRAMEX\"][\"tweets\"])\n",
        "all_BELJAP_tweets = \". \".join(world_cup_games[\"BELJAP\"][\"tweets\"])\n",
        "all_SWESUI_tweets = \". \".join(world_cup_games[\"SWESUI\"][\"tweets\"])\n",
        "all_COLENG_tweets = \". \".join(world_cup_games[\"COLENG\"][\"tweets\"])\n",
        "all_BEFORE_tweets = \". \".join(tweets_by_date[\"2018-06-29\"])\n",
        "print(\"length of all_FRACRO_tweets = \" + str(len(all_FRACRO_tweets)))\n",
        "print(\"length of all_CROENG_tweets = \" + str(len(all_CROENG_tweets)))\n",
        "print(\"length of all_BRAMEX_tweets = \" + str(len(all_BRAMEX_tweets)))\n",
        "print(\"length of all_BELJAP_tweets = \" + str(len(all_BELJAP_tweets)))\n",
        "print(\"length of all_SWESUI_tweets = \" + str(len(all_SWESUI_tweets)))\n",
        "print(\"length of all_COLENG_tweets = \" + str(len(all_COLENG_tweets)))\n",
        "print(\"length of all_BEFORE_tweets = \" + str(len(all_BEFORE_tweets)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of all_FRACRO_tweets = 1500569\n",
            "length of all_CROENG_tweets = 969233\n",
            "length of all_BRAMEX_tweets = 112269\n",
            "length of all_BELJAP_tweets = 112269\n",
            "length of all_SWESUI_tweets = 247862\n",
            "length of all_COLENG_tweets = 247862\n",
            "length of all_BEFORE_tweets = 13044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y8jF1ou2dfL"
      },
      "source": [
        "### 2. Events Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr7f8BKqBSLE"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
        "from string import punctuation\r\n",
        "from heapq import nlargest\r\n",
        "from collections import defaultdict \r\n",
        "from textblob import TextBlob\r\n",
        "\r\n",
        "stopwords = list(STOP_WORDS)\r\n",
        "nlp = spacy.load('en')\r\n",
        "nlp.max_length = 1600000 # don't increase this or program will crash\r\n",
        "\r\n",
        "def events_score(sent):\r\n",
        "  score = 0\r\n",
        "  if \"score\" in sent or \"goal\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"red card\" in sent or \"penal\" in sent or \"pk\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"yellow card\" in sent or \"freekick\" in sent or \"booked\" in sent or \"booking\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"foul\" in sent or \"sub\" in sent:\r\n",
        "    score += 2\r\n",
        "  if \"half\" in sent or \"full\" in sent or \"over\" in sent or \"extra\" in sent or \"ht\" in sent or \"ft\" in sent:\r\n",
        "    score += 2\r\n",
        "  return score \r\n",
        "\r\n",
        "\r\n",
        "# Place All As A Function For Reuseability\r\n",
        "def text_summarizer(raw_docx):\r\n",
        "    raw_text = raw_docx\r\n",
        "    docx = nlp(raw_text)\r\n",
        "    stopwords = list(STOP_WORDS)\r\n",
        "    # Build Word Frequency\r\n",
        "    # word.text is tokenization in spacy\r\n",
        "    word_frequencies = defaultdict(int)  \r\n",
        "    for word in docx:  \r\n",
        "      if word.text not in stopwords:\r\n",
        "        word_frequencies[word.text] += 1\r\n",
        "\r\n",
        "\r\n",
        "    maximum_frequncy = max(word_frequencies.values())\r\n",
        "\r\n",
        "    for word in word_frequencies.keys():  \r\n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\r\n",
        "    # Sentence Tokens\r\n",
        "    sentence_list = [ sentence for sentence in docx.sents ]\r\n",
        "\r\n",
        "    # Calculate Sentence Score and Ranking\r\n",
        "    sentence_scores = defaultdict(int)\r\n",
        "    for sent in sentence_list:\r\n",
        "      sentence = sent.text.lower()\r\n",
        "      if len(sentence.split(' ')) < 20 and abs(TextBlob(sentence).sentiment.polarity) <= 0.5:     \r\n",
        "      # if len(sentence) < 120 and abs(TextBlob(sentence).sentiment.polarity) > 0.5:\r\n",
        "        sentence_scores[sent] = events_score(sentence) \r\n",
        "        for word in sent:\r\n",
        "          if word.text.lower() in word_frequencies.keys():\r\n",
        "            sentence_scores[sent] += word_frequencies[word.text.lower()]\r\n",
        "\r\n",
        "    # Find N Largest\r\n",
        "    summary_sentences = nlargest(5, sentence_scores, key=sentence_scores.get)\r\n",
        "    final_sentences = [ w.text for w in summary_sentences ]\r\n",
        "    summary = '\\n'.join(final_sentences)\r\n",
        "    #print(\"Original Document\\n\")\r\n",
        "    #print(raw_docx)\r\n",
        "    #print(\"Total Length:\",len(raw_docx))\r\n",
        "    #print('\\n\\nSummarized Document\\n')\r\n",
        "    #print(\"Summary Length:\",len(summary))\r\n",
        "    #print(\"Summary: \" + summary)\r\n",
        "    return summary"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FdiIhqi2lkC"
      },
      "source": [
        "### 3. Summarization Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP9AjI8-dRNJ",
        "outputId": "366f04d6-1228-4de8-fd2c-2efb32645bf9"
      },
      "source": [
        "# text_summarizer(all_BEFORE_tweets)\n",
        "print(\"Summary of BRAMEX: \" + text_summarizer(all_BRAMEX_tweets))\n",
        "print(\"Summary of COLENG: \" + text_summarizer(all_COLENG_tweets))\n",
        "print(\"Summary of CROENG: \" + text_summarizer(all_CROENG_tweets))\n",
        "print(\"Summary of FRACRO: \" + text_summarizer(all_FRACRO_tweets))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of BRAMEX: world cup croatia dump denmark in breathtaking penalty shootout with record saves.\n",
            "what night of games denmark have done incredibly well to push croatia to penalties after squeezing into the knockouts while russia continue to ride the momentum.\n",
            "has qualified for the quarter finals of the after penalty shoot out win over photo vcg.\n",
            "win on penalties to advance to the quarter finals are out full stats and ratings.\n",
            "sergei ignashevich is the oldest ever player to score an own goal in at the age of years days htt.\n",
            "Summary of COLENG: for the first time in history england win penalty shootout at fifa world cup eric dier scores the winner as the.\n",
            "england win on penalties and spurs player hits said winning penalty this might be england year after all.\n",
            "tell the players to write our own history to tell our own stories we don have to be bound by history manager gareth southgate after winning the first penalty shootout at the years after his own missed penalty led to england exit from the euros.\n",
            "who would have thought that england could win penalty shoot out in world cup what night at the.\n",
            "june players ride july england finally win penalty shootout at the discover the power of unicorns for yourself join our rugby blessing.\n",
            "Summary of CROENG: croatia and england have switched roles after that goal englands been awful after that goal in all aspects.\n",
            "lmao now that croatia scored england gonna start attacking and will be cought on counter.\n",
            "england have to make goal dont think they can win penalty shootouts in row.\n",
            "england fans in london went absolutely nuts after kieran trippiers incredible goal made it watch.\n",
            "england must avoid penalties at all cost the croatian goalkeeper is just good penalty stopper.\n",
            "Summary of FRACRO: minutes to go one own goal one penalty goal one gift of goal and three wonderful goals.\n",
            "an own goal good finish off an improvised free kick penalty for handball after var review all within minute.\n",
            "well that did have everything an own goal controversial penalty var stage invasion and plenty of.\n",
            "a sublime france with the exciting new young star kylian mbappe lifted the trophy today with victory over rugged croatia the best team in the world just like tg is the best.\n",
            "the controversial penalty award also changed the dynamics of the match croatia played better than france but france won congratulations to france.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfOnQo-Mqzz1",
        "outputId": "5a079ca1-620f-4e4b-e6d0-bff4acf560ff"
      },
      "source": [
        "sett = \"that pernicious foul smell permeating through twitter is shitting himself after that goal three lions one.\"\n",
        "TextBlob(sett).sentiment.polarity"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}